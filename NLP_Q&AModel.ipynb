{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbzbSDKOVYafKzQUIriD2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyamzzz/GISMA/blob/main/NLP_Q%26AModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction: The objective of this model is to build a Question Answering system which can automatically provide answers to questions based on a given context or knowledge base. The objective of the QA model is to understand the meaning of both the question and the provided context and generate accurate and relevant answers.\n",
        "\n",
        "## In a business problem-solving context, the QA model can be immensely valuable. It enables businesses to leverage the power of language understanding to extract valuable information from large amounts of textual data. By automating the process of finding answers to questions, the QA model can save time, improve efficiency, and enhance decision-making processes.\n",
        "\n",
        "### NOTE: THIS PIPELINE MIGHT REQUIRE HIGHER GPU SETTING TO RUN DEPENDING ON YOUR SYSTEM CAPABILITIES. TO ADJUST THAT IN GOOGLE COLAB, PLEASE GO TO RUNTIME> CHANGE RUNTIME TYPE> HARDWARE ACCELERATOR> GPU"
      ],
      "metadata": {
        "id": "34neEfhy_B4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regarding the SQUaD Dataset - The SQuAD dataset consists of a large collection of Wikipedia articles paired with crowdsourced questions and their corresponding answers. The dataset covers a diverse range of topics, including history, science, literature, and more. It includes both factoid and descriptive types of questions, allowing for a comprehensive evaluation of QA systems. The dataset has been downloaded from the below link:\n",
        "\n",
        "Link: https://rajpurkar.github.io/SQuAD-explorer/"
      ],
      "metadata": {
        "id": "tHOTnnMNC6pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Loading the Training dataset"
      ],
      "metadata": {
        "id": "M8H7cvS0Emjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6H2YC73LmU_"
      },
      "outputs": [],
      "source": [
        "import json # function for working with json data\n",
        "import pandas as pd # data manaipluation and analysis\n",
        "\n",
        "# Load the training set\n",
        "with open('train-v2.0.json', 'r') as f:\n",
        "    train_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preparing the dataset for training\n",
        "\n",
        "#### The below code makes three empty lists - \"contexts\", \"questions\" and \"answers\" which will store the extracted context, question, and answer data from the SQuAD dataset.\n",
        "\n",
        "#### Since the dataset was too big, I will be working on the subset of the data. The code creates a subset of the data by randomly sampling 1000 rows using the sample method. The random_state parameter ensures reproducibility of the sampling results.\n",
        "\n"
      ],
      "metadata": {
        "id": "41-hymvDEwrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Initialize lists to store the contexts, questions, and answers\n",
        "contexts = []\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "# Loop over the data\n",
        "for article in train_data['data']:\n",
        "    for paragraph in article['paragraphs']:\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                # Append the context, question, and answer to the respective lists\n",
        "                contexts.append(context)\n",
        "                questions.append(question)\n",
        "                answers.append(answer)\n",
        "\n",
        "# Convert the lists into a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'context': contexts,\n",
        "    'question': questions,\n",
        "    'answer_start': [answer['answer_start'] for answer in answers],\n",
        "    'text': [answer['text'] for answer in answers]\n",
        "})\n",
        "\n",
        "# Create a subset of the data\n",
        "df_subset = df.sample(n=1000, random_state=1)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(df_subset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWERmox8P7Fu",
        "outputId": "7ebafc1a-da0b-4eaf-df2a-d1b6630f0ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 context  \\\n",
            "31373  Despite the death of Queen Mary on 24 March, t...   \n",
            "37550  Clothing can and has in history been made from...   \n",
            "7717   In 2013â€“14 a pornographic actor was trying to ...   \n",
            "71479  The Cold War drew to a close in the late 1980s...   \n",
            "36594  Commercial turkeys are usually reared indoors ...   \n",
            "\n",
            "                                                question  answer_start  \\\n",
            "31373     When was the coronation of Elizabeth as Queen?            63   \n",
            "37550  What is an article that is carried rather than...           293   \n",
            "7717   What legal system did the actor use after fili...           152   \n",
            "71479  What was  the Soviet Union suffering from in t...           224   \n",
            "36594  What the average for the amount of  turkeys ar...           880   \n",
            "\n",
            "                                        text  \n",
            "31373                            2 June 1953  \n",
            "37550                                 purses  \n",
            "7717                 Federal Court of Canada  \n",
            "71479             severe economic stagnation  \n",
            "36594  60 million birds in the United States  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Loading the BERT Model and Tokenizer\n",
        "\n",
        "#### transformers library provides a high-level interface for working with various pre-trained models in the field of natural language processing.\n",
        "\n",
        "#### BertForQuestionAnswering and BertTokenizer classes provide the BERT model architecture and tokenizer respectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "3JKyYaB8GlA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "# Load the BERT model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ0q4jFj6Bh8",
        "outputId": "4705f4f9-59f6-40fa-8679-9ec2780d18ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Tokenizing and Encoding the Data\n",
        "\n",
        "#### The below code tokenizes and encodes the questions and contexts from the DataFrame subset (df_subset). It also converts the answer spans from character positions to token positions.\n",
        "\n",
        "#### The purpose of this code is to tokenize and encode the questions and contexts from the DataFrame subset, as well as convert the answer spans from character positions to token positions. By encoding the data in this way, it becomes compatible with BERT-based models and can be used for training or inference in question answering tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "oiAJzD-DJZbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the questions and contexts\n",
        "encodings = tokenizer(df_subset['question'].tolist(), df_subset['context'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Convert the answer spans from character positions to token positions\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "for i in range(len(df_subset)):\n",
        "    start_char = df_subset['answer_start'].iloc[i]\n",
        "    end_char = start_char + len(df_subset['text'].iloc[i])\n",
        "    start_token = len(tokenizer(df_subset['question'].iloc[i], df_subset['context'].iloc[i][:start_char])['input_ids'])\n",
        "    end_token = len(tokenizer(df_subset['question'].iloc[i], df_subset['context'].iloc[i][:end_char])['input_ids']) - 1\n",
        "    start_positions.append(start_token)\n",
        "    end_positions.append(end_token)\n",
        "\n",
        "# Add the start and end positions to the encodings\n",
        "encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohARMDgt636F",
        "outputId": "9456b0c3-e633-4f24-d6c4-3dbd34f8ec67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Creating a Custom Dataset for Question Answering\n",
        "\n",
        "#### The below code defines a custom dataset class called SquadDataset and uses it to convert the encoded data into a PyTorch dataset. The purpose of this code is to define a custom dataset class called SquadDataset and use it to convert the encoded data (encodings) into a PyTorch dataset (dataset). By creating a custom dataset, we can easily work with the encoded data during training or inference, allowing us to leverage the functionality and benefits provided by PyTorch's data handling capabilities."
      ],
      "metadata": {
        "id": "5agmCC1CKrJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# Convert our encodings into a Dataset\n",
        "dataset = SquadDataset(encodings)\n"
      ],
      "metadata": {
        "id": "5G9ZwmIm7Ihw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Training Loop with DataLoader and Optimizer\n",
        "\n",
        "#### The below code demonstrates the training loop for a question answering model using PyTorch and the Transformers library. DataLoader is used to create a data loader that loads the dataset in batches during training, and AdamW is an optimizer that implements the Adam algorithm with weight decay.\n",
        "\n",
        "#### Furthermore, The code sets the device for training based on the availability of a CUDA-compatible GPU. If a GPU is available, the device is set to 'cuda'; otherwise, it is set to 'cpu'. This allows the model and data to be moved to the appropriate device for efficient computation. The loss value at the end of each epoch is printed to monitor the training progress and assess the model's performance.\n",
        "\n",
        "#### Overall, the code is responsible for training a question answering model using the BERT-based architecture. It handles the data loading, model initialization, optimization setup, and implements the training loop to update the model's parameters based on the computed gradients. The goal is to minimize the loss and improve the model's ability to answer questions accurately."
      ],
      "metadata": {
        "id": "CNp0N9RpNVu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "\n",
        "# Set the batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):  # Number of epochs\n",
        "    for batch in dataloader:\n",
        "        # Move the batch tensors to the same device as the model\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f'Loss at epoch {epoch}: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T15GH817Km7",
        "outputId": "5b58c739-bc6d-4829-a14b-a72f583c10ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch 0: 4.35087251663208\n",
            "Loss at epoch 1: 3.1491639614105225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Generating answers for given questions\n",
        "\n",
        "#### The below code demonstrates how to use the trained question answering model to generate answers for a list of given questions. A context is defined, which represents a piece of text or document from which the questions will be answered. In this case, the context is a description of the Python programming language but you can always choose other contexts and questions depending on the uploaded dataset.\n",
        "\n",
        "#### The purpose of this code is to demonstrate the use of a  the trained question answering model and the tokenizer to generate answers for a list of given questions based on the provided context. It showcases the process of encoding the context and question, obtaining predictions from the model, and extracting and decoding the answer from the predicted tokens.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v4LrfjX_P5Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model back to CPU for testing\n",
        "model.to('cpu')\n",
        "\n",
        "# Define a context\n",
        "context = \"The University of California, Berkeley (UC Berkeley, Berkeley, Cal, or California) is a public research university in Berkeley, California. Founded in 1868, Berkeley is the oldest of the ten research universities affiliated with the University of California system, and is often cited as the top public university in the United States and around the world.\"\n",
        "\n",
        "# Define a list of questions\n",
        "questions = [\n",
        "         \"When was the University of California, Berkeley founded?\",\n",
        "    \"What is the University of California, Berkeley often cited as?\",\n",
        "    \"Where is the University of California, Berkeley located?\"\n",
        "]\n",
        "\n",
        "# Create a list to store the answers\n",
        "answers = []\n",
        "\n",
        "# Iterate over the questions\n",
        "for question in questions:\n",
        "    # Encode the context and question\n",
        "    inputs = tokenizer(question, context, return_tensors='pt')\n",
        "\n",
        "    # Get the model's predictions\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Get the start and end scores\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # Get the start and end positions\n",
        "    start_position = torch.argmax(start_scores)\n",
        "    end_position = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the answer\n",
        "    tokens = inputs.input_ids[0].tolist()\n",
        "    answer_tokens = tokens[start_position : end_position+1]\n",
        "    answer = tokenizer.decode(answer_tokens)\n",
        "\n",
        "    # Append the answer to the list\n",
        "    answers.append(answer)\n",
        "# Print the answers\n",
        "for i, question in enumerate(questions):\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answers[i]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMk_ilLOEM6h",
        "outputId": "d6d3c56c-3989-4437-9ef0-c745d0cb3c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: When was the University of California, Berkeley founded?\n",
            "Answer: of california, berkeley (\n",
            "\n",
            "Question: What is the University of California, Berkeley often cited as?\n",
            "Answer: of california, berkeley (\n",
            "\n",
            "Question: Where is the University of California, Berkeley located?\n",
            "Answer: of california, berkeley (\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that using 'bert-base-uncased model' is predicting answers but they are not very accurate so let's try'bert-large-uncased-whole-word-masking-finetuned-squad' model. This model has already been fine-tuned on the SQuAD dataset, so hopefully it should perform better on the same given context and questions."
      ],
      "metadata": {
        "id": "Ryerw6CWfIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Tokenize the questions and contexts\n",
        "encodings = tokenizer(df['question'].tolist(), df['context'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Convert the answer spans from character positions to token positions\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "for i in range(len(df)):\n",
        "    start_char = df['answer_start'][i]\n",
        "    end_char = start_char + len(df['text'][i])\n",
        "    start_token = len(tokenizer(df['question'][i], df['context'][i][:start_char])['input_ids'])\n",
        "    end_token = len(tokenizer(df['question'][i], df['context'][i][:end_char])['input_ids']) - 1\n",
        "    start_positions.append(start_token)\n",
        "    end_positions.append(end_token)\n",
        "\n",
        "# Add the start and end positions to the encodings\n",
        "encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu28dIYrQ9tQ",
        "outputId": "bb3be9dc-cf5b-4bb1-c9bc-100b3d119a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"The University of California, Berkeley (UC Berkeley, Berkeley, Cal, or California) is a public research university in Berkeley, California. Founded in 1868, Berkeley is the oldest of the ten research universities affiliated with the University of California system, and is often cited as the top public university in the United States and around the world.\"\n",
        "questions = [\n",
        "    \"When was the University of California, Berkeley founded?\",\n",
        "    \"What is the University of California, Berkeley often cited as?\",\n",
        "    \"Where is the University of California, Berkeley located?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, context, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2KAiM9rc7Mc",
        "outputId": "4c1b00ec-805b-49c8-d3fe-4fa7546062d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: When was the University of California, Berkeley founded?\n",
            "Answer: 1868\n",
            "\n",
            "Question: What is the University of California, Berkeley often cited as?\n",
            "Answer: the top public university in the united states and around the world\n",
            "\n",
            "Question: Where is the University of California, Berkeley located?\n",
            "Answer: berkeley, california\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "**Summary**:\n",
        "\n",
        "I started with a BERT-based Question Answering model trained on the SQuAD dataset. The model was initially using the 'bert-base-uncased' variant of BERT. However, the model was not performing well, and the answers to the questions were not accurate.\n",
        "\n",
        "To improve the model, I decided to use a BERT model that has already been fine-tuned on a question-answering task. For that I chose the 'bert-large-uncased-whole-word-masking-finetuned-squad' model. This model is a larger version of BERT and has been trained with whole-word masking, which is a more advanced form of masking that can lead to better performance. Moreover, the model has been fine-tuned on the SQuAD dataset.\n",
        "\n",
        "After replacing the model, I tested it with the same questions based on a context which were used for 'bert-base-uncased' model to see if it can provide better answers. The model was able to answer the questions correctly, indicating that it has improved.\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "The use of a pre-trained model that has been fine-tuned on a similar task can significantly improve the performance of the model. The 'bert-large-uncased-whole-word-masking-finetuned-squad' model performed better than the 'bert-base-uncased' model on the question-answering task.\n",
        "\n",
        "*P.S. Thank you for teaching us this course and showing the immense posibilities of the NLP field. I thoroughly enjoyed the course and look forward to building more NLP based models.*"
      ],
      "metadata": {
        "id": "cgwwgLijgHJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html M508B.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJK54U6YmQGA",
        "outputId": "abeb78ba-8b0e-4374-8a49-37807e29722d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook M508B.ipynb to html\n",
            "[NbConvertApp] Writing 1069283 bytes to M508B.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oOVv3RKpgz2A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}